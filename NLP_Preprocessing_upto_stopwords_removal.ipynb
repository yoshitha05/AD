{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshitha05/AD/blob/main/NLP_Preprocessing_upto_stopwords_removal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n"
      ],
      "metadata": {
        "id": "Vn1dvo8K1d4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lO8F4SQL2RyG",
        "outputId": "2f3a90d0-272f-4616-962b-d75618f2c92f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "dnopDC2GGYXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"Hello Welcome,to MallaReddy NLP Classes.\n",
        "Please do pratice the entire course! to become expert in NLP.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NKVZom7p2Zxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbcGUtGs2Zwk",
        "outputId": "f07488db-fd6e-4b13-f854-813f9875abd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to MallaReddy NLP Classes.\n",
            "Please do pratice the entire course! to become expert in NLP.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##  Tokenization\n",
        "## Sentence-->paragraphs\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "zNKj7kbX2ZjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt_tab') #required for tokenization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boOhG3gN24S2",
        "outputId": "1233a813-b8c7-4e7e-b0cf-d60e2a894cb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizing sentences\n",
        "sentences=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "L00dlvxz2YWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBsh7uo71mpH",
        "outputId": "f5da53e6-2b29-4649-c9f1-4fa0690113df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f6Zjx103IoE",
        "outputId": "da0442db-820f-42fe-fd6d-422938e49356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to MallaReddy NLP Classes.\n",
            "Please do pratice the entire course!\n",
            "to become expert in NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization  words\n",
        "## Paragraph-->words\n",
        "## sentence--->words\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "FCW6Mgdb3IpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "cViF2n0g3ItU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxKIgtM-3Iup",
        "outputId": "c6d64912-9bcd-4adb-c895-a493e9dcf97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KTQGgnS3eCG",
        "outputId": "ce79d876-c715-4521-9eda-7761719040b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            "Welcome\n",
            ",\n",
            "to\n",
            "MallaReddy\n",
            "NLP\n",
            "Classes\n",
            ".\n",
            "Please\n",
            "do\n",
            "pratice\n",
            "the\n",
            "entire\n",
            "course\n",
            "!\n",
            "to\n",
            "become\n",
            "expert\n",
            "in\n",
            "NLP\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WordPunctTokenizer**"
      ],
      "metadata": {
        "id": "qqscKf5m4Wee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the help of nltk.tokenize.WordPunctTokenizer() method, we are able to extract the tokens from string of words or sentences in the form of Alphabetic and Non-Alphabetic character by using tokenize.WordPunctTokenizer()() method.\n",
        "\n"
      ],
      "metadata": {
        "id": "9sM2eJl54QpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# Create a reference variable for Class WordPunctTokenizer\n",
        "tk = WordPunctTokenizer()\n",
        "\n",
        "# Create a string input\n",
        "text = \"Mallareddy...$$&* \\college\\of engineering.ac.in 123456.987\"\n",
        "\n",
        "# Use tokenize method\n",
        "output = tk.tokenize(text)\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDsoume-3eHx",
        "outputId": "550828fa-3e4f-4005-b36c-9d2a2afe10b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mallareddy', '...$$&*', '\\\\', 'college', '\\\\', 'of', 'engineering', '.', 'ac', '.', 'in', '123456', '.', '987']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Stemming**\n",
        "Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma. Stemming is important in natural language understanding (NLU) and natural language processing (NLP)."
      ],
      "metadata": {
        "id": "dLKf6A9v5Fsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Porter** **Stemmer**"
      ],
      "metadata": {
        "id": "cJqbMuV456Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Classification Problem\n",
        "## Comments of product is a positive review or negative review\n",
        "## Reviews----> [eating, eat,eaten]---->eat [going,gone,goes]--->go\n",
        "\n",
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ],
      "metadata": {
        "id": "Wq04meMn3eJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "M3ei7v_I3eOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "ZqDVZSM_6KCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications for Stemming:**\n",
        "\n",
        "1. Search Engines & Information Retrieval\n",
        "\n",
        "          Google, Bing, or Elasticsearch use stemming to match user queries with documents.\n",
        "          Example: Searching for \"running\" should return documents with \"run,\" \"runs,\" \"runner.\"\n",
        "\n",
        "2. Spam Detection & Email Filtering\n",
        "\n",
        "          Words like \"clicking\" and \"clicked\" are reduced to \"click\" for pattern matching.\n",
        "\n",
        "3. Chatbots & Virtual Assistants\n",
        "\n",
        "          Simplifies user queries by reducing words to a common base.\n",
        "          Example: \"booking\" → \"book\" in a travel chatbot.\n",
        "4. Topic Modeling & Keyword Extraction\n",
        "\n",
        "          Helps in extracting common themes from text (e.g., LDA topic modeling).\n",
        "5. Text Indexing & Database Queries\n",
        "\n",
        "          Improves efficiency by reducing word variations in databases."
      ],
      "metadata": {
        "id": "r9mxiVSN7LDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF8drpqr6MFM",
        "outputId": "97f17209-bd48-4a64-9fdb-66932b1c086f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LjX2rxR17xbs",
        "outputId": "cb9e0c1c-5f27-4c2a-b1b4-a4da8b0aff36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stemming.stem(\"sitting\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W4qEKSBw7xjU",
        "outputId": "cb6de799-23fa-4935-c84f-31cd3d44e425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RegexpStemmer class**\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example\n",
        "\n"
      ],
      "metadata": {
        "id": "j4-OzgL27460"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "G_XxnUQo7xkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4) # min parameter use was lesserthan that length of string no stemming"
      ],
      "metadata": {
        "id": "vxjlD2MZ7xpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zb0dJRTr9eea",
        "outputId": "5c456c08-61c0-4071-9da3-caa9d5e086da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sh923op-9jGD",
        "outputId": "dd50a672-02a9-48cd-f7f1-54d38de56a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemmer**\n",
        "\n",
        "\n",
        "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer. support multiple languages"
      ],
      "metadata": {
        "id": "EmxJ9A969zLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "h74alyaW93Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "Crc622Vi91mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+snowballsstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7-8e7Jj-C-i",
        "outputId": "9ce2b96c-763a-45b5-cbf8-a426c89dd8c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Porter stemmer\n",
        "\n",
        "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si6pYhGn-FSN",
        "outputId": "9245b2d8-8899-4286-e94d-999388f810f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#snowball stemmer\n",
        "\n",
        "snowballsstemmer.stem(\"fairly\"),snowballsstemmer.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GkydKXI-UB2",
        "outputId": "e93ff03a-7213-42a2-fb31-9ff64144d1e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Wordnet Lemmatizer**\n",
        "\n",
        "\n",
        "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
        "\n",
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example −"
      ],
      "metadata": {
        "id": "RIav9lFf-o3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Q&A,chatbots,text summarization\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "mRVlgJ5P-RDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "JVqwO1tf-tIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GavKQzzAXR5",
        "outputId": "70a9eaed-698a-4e99-d9a9-e127493cba50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#   POS- Noun-n\n",
        "#  verb-v\n",
        "#  adjective-a\n",
        "#  adverb-r\n",
        "\n"
      ],
      "metadata": {
        "id": "afXir7ib-1NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"going\",pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I9Hfsies9jHU",
        "outputId": "0b7d258a-d448-4c39-82b3-09eb36fe1448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n"
      ],
      "metadata": {
        "id": "n8AjEcJDGogV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEEMzGxLGpu2",
        "outputId": "173bfb73-df32-4c57-da53-cb27496e8c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"goes\",pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8xLdswWyGsaK",
        "outputId": "bcabef80-b733-4db3-f434-89c0d0a402d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"fairly\",pos='v'),lemmatizer.lemmatize(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odTXvw12GyZ_",
        "outputId": "0e733565-555b-4d1b-8cd8-1cdf2c4174c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairly', 'sportingly')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ysWssS5fHHAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applications for Lemmatization:**\n",
        "\n",
        "1. Machine Translation (Google Translate, DeepL)\n",
        "\n",
        "      Ensures the correct word form is used in translations.\n",
        "      Example: \"was\" → \"be\" (helps with accurate sentence structure).\n",
        "\n",
        "2. Sentiment Analysis & Opinion Mining\n",
        "\n",
        "      Ensures words retain their proper meaning in text analysis.\n",
        "      Example: \"better\" → \"good\" (sentiment analysis should recognize this relation).\n",
        "\n",
        "3. Named Entity Recognition (NER)\n",
        "\n",
        "      Helps identify proper nouns correctly.\n",
        "      Example: \"went\" → \"go\" ensures correct tagging in NLP models.\n",
        "\n",
        "4. Text Summarization & Question-Answering Systems\n",
        "\n",
        "      Ensures grammatically correct sentence formation.\n",
        "\n",
        "5. Speech Recognition & Voice Assistants (Siri, Alexa, Google Assistant)\n",
        "\n",
        "      Ensures correct grammar and sentence formation in responses.\n",
        "\n",
        "6. Legal & Medical Document Processing\n",
        "\n",
        "    High accuracy is required in processing contracts, patient records, etc."
      ],
      "metadata": {
        "id": "x-t5OcMo9CQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stopwords**"
      ],
      "metadata": {
        "id": "HtpKf0d8HMG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#speech of abdul kalam garu from wikipedia\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "nEbPd8ltHRDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Oun9xtorHZjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a8P8SArIVZO",
        "outputId": "28a3045f-c405-4b10-9e15-e8ae3bb5ea19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6Y0-lqQIVbZ",
        "outputId": "ef25c168-b767-43c9-9186-d3a584bfcea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('arabic')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ENybUPxIepS",
        "outputId": "ad3720d6-981d-4347-e68c-82ba367bd4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['إذ',\n",
              " 'إذا',\n",
              " 'إذما',\n",
              " 'إذن',\n",
              " 'أف',\n",
              " 'أقل',\n",
              " 'أكثر',\n",
              " 'ألا',\n",
              " 'إلا',\n",
              " 'التي',\n",
              " 'الذي',\n",
              " 'الذين',\n",
              " 'اللاتي',\n",
              " 'اللائي',\n",
              " 'اللتان',\n",
              " 'اللتيا',\n",
              " 'اللتين',\n",
              " 'اللذان',\n",
              " 'اللذين',\n",
              " 'اللواتي',\n",
              " 'إلى',\n",
              " 'إليك',\n",
              " 'إليكم',\n",
              " 'إليكما',\n",
              " 'إليكن',\n",
              " 'أم',\n",
              " 'أما',\n",
              " 'أما',\n",
              " 'إما',\n",
              " 'أن',\n",
              " 'إن',\n",
              " 'إنا',\n",
              " 'أنا',\n",
              " 'أنت',\n",
              " 'أنتم',\n",
              " 'أنتما',\n",
              " 'أنتن',\n",
              " 'إنما',\n",
              " 'إنه',\n",
              " 'أنى',\n",
              " 'أنى',\n",
              " 'آه',\n",
              " 'آها',\n",
              " 'أو',\n",
              " 'أولاء',\n",
              " 'أولئك',\n",
              " 'أوه',\n",
              " 'آي',\n",
              " 'أي',\n",
              " 'أيها',\n",
              " 'إي',\n",
              " 'أين',\n",
              " 'أين',\n",
              " 'أينما',\n",
              " 'إيه',\n",
              " 'بخ',\n",
              " 'بس',\n",
              " 'بعد',\n",
              " 'بعض',\n",
              " 'بك',\n",
              " 'بكم',\n",
              " 'بكم',\n",
              " 'بكما',\n",
              " 'بكن',\n",
              " 'بل',\n",
              " 'بلى',\n",
              " 'بما',\n",
              " 'بماذا',\n",
              " 'بمن',\n",
              " 'بنا',\n",
              " 'به',\n",
              " 'بها',\n",
              " 'بهم',\n",
              " 'بهما',\n",
              " 'بهن',\n",
              " 'بي',\n",
              " 'بين',\n",
              " 'بيد',\n",
              " 'تلك',\n",
              " 'تلكم',\n",
              " 'تلكما',\n",
              " 'ته',\n",
              " 'تي',\n",
              " 'تين',\n",
              " 'تينك',\n",
              " 'ثم',\n",
              " 'ثمة',\n",
              " 'حاشا',\n",
              " 'حبذا',\n",
              " 'حتى',\n",
              " 'حيث',\n",
              " 'حيثما',\n",
              " 'حين',\n",
              " 'خلا',\n",
              " 'دون',\n",
              " 'ذا',\n",
              " 'ذات',\n",
              " 'ذاك',\n",
              " 'ذان',\n",
              " 'ذانك',\n",
              " 'ذلك',\n",
              " 'ذلكم',\n",
              " 'ذلكما',\n",
              " 'ذلكن',\n",
              " 'ذه',\n",
              " 'ذو',\n",
              " 'ذوا',\n",
              " 'ذواتا',\n",
              " 'ذواتي',\n",
              " 'ذي',\n",
              " 'ذين',\n",
              " 'ذينك',\n",
              " 'ريث',\n",
              " 'سوف',\n",
              " 'سوى',\n",
              " 'شتان',\n",
              " 'عدا',\n",
              " 'عسى',\n",
              " 'عل',\n",
              " 'على',\n",
              " 'عليك',\n",
              " 'عليه',\n",
              " 'عما',\n",
              " 'عن',\n",
              " 'عند',\n",
              " 'غير',\n",
              " 'فإذا',\n",
              " 'فإن',\n",
              " 'فلا',\n",
              " 'فمن',\n",
              " 'في',\n",
              " 'فيم',\n",
              " 'فيما',\n",
              " 'فيه',\n",
              " 'فيها',\n",
              " 'قد',\n",
              " 'كأن',\n",
              " 'كأنما',\n",
              " 'كأي',\n",
              " 'كأين',\n",
              " 'كذا',\n",
              " 'كذلك',\n",
              " 'كل',\n",
              " 'كلا',\n",
              " 'كلاهما',\n",
              " 'كلتا',\n",
              " 'كلما',\n",
              " 'كليكما',\n",
              " 'كليهما',\n",
              " 'كم',\n",
              " 'كم',\n",
              " 'كما',\n",
              " 'كي',\n",
              " 'كيت',\n",
              " 'كيف',\n",
              " 'كيفما',\n",
              " 'لا',\n",
              " 'لاسيما',\n",
              " 'لدى',\n",
              " 'لست',\n",
              " 'لستم',\n",
              " 'لستما',\n",
              " 'لستن',\n",
              " 'لسن',\n",
              " 'لسنا',\n",
              " 'لعل',\n",
              " 'لك',\n",
              " 'لكم',\n",
              " 'لكما',\n",
              " 'لكن',\n",
              " 'لكنما',\n",
              " 'لكي',\n",
              " 'لكيلا',\n",
              " 'لم',\n",
              " 'لما',\n",
              " 'لن',\n",
              " 'لنا',\n",
              " 'له',\n",
              " 'لها',\n",
              " 'لهم',\n",
              " 'لهما',\n",
              " 'لهن',\n",
              " 'لو',\n",
              " 'لولا',\n",
              " 'لوما',\n",
              " 'لي',\n",
              " 'لئن',\n",
              " 'ليت',\n",
              " 'ليس',\n",
              " 'ليسا',\n",
              " 'ليست',\n",
              " 'ليستا',\n",
              " 'ليسوا',\n",
              " 'ما',\n",
              " 'ماذا',\n",
              " 'متى',\n",
              " 'مذ',\n",
              " 'مع',\n",
              " 'مما',\n",
              " 'ممن',\n",
              " 'من',\n",
              " 'منه',\n",
              " 'منها',\n",
              " 'منذ',\n",
              " 'مه',\n",
              " 'مهما',\n",
              " 'نحن',\n",
              " 'نحو',\n",
              " 'نعم',\n",
              " 'ها',\n",
              " 'هاتان',\n",
              " 'هاته',\n",
              " 'هاتي',\n",
              " 'هاتين',\n",
              " 'هاك',\n",
              " 'هاهنا',\n",
              " 'هذا',\n",
              " 'هذان',\n",
              " 'هذه',\n",
              " 'هذي',\n",
              " 'هذين',\n",
              " 'هكذا',\n",
              " 'هل',\n",
              " 'هلا',\n",
              " 'هم',\n",
              " 'هما',\n",
              " 'هن',\n",
              " 'هنا',\n",
              " 'هناك',\n",
              " 'هنالك',\n",
              " 'هو',\n",
              " 'هؤلاء',\n",
              " 'هي',\n",
              " 'هيا',\n",
              " 'هيت',\n",
              " 'هيهات',\n",
              " 'والذي',\n",
              " 'والذين',\n",
              " 'وإذ',\n",
              " 'وإذا',\n",
              " 'وإن',\n",
              " 'ولا',\n",
              " 'ولكن',\n",
              " 'ولو',\n",
              " 'وما',\n",
              " 'ومن',\n",
              " 'وهو',\n",
              " 'يا',\n",
              " 'أبٌ',\n",
              " 'أخٌ',\n",
              " 'حمٌ',\n",
              " 'فو',\n",
              " 'أنتِ',\n",
              " 'يناير',\n",
              " 'فبراير',\n",
              " 'مارس',\n",
              " 'أبريل',\n",
              " 'مايو',\n",
              " 'يونيو',\n",
              " 'يوليو',\n",
              " 'أغسطس',\n",
              " 'سبتمبر',\n",
              " 'أكتوبر',\n",
              " 'نوفمبر',\n",
              " 'ديسمبر',\n",
              " 'جانفي',\n",
              " 'فيفري',\n",
              " 'مارس',\n",
              " 'أفريل',\n",
              " 'ماي',\n",
              " 'جوان',\n",
              " 'جويلية',\n",
              " 'أوت',\n",
              " 'كانون',\n",
              " 'شباط',\n",
              " 'آذار',\n",
              " 'نيسان',\n",
              " 'أيار',\n",
              " 'حزيران',\n",
              " 'تموز',\n",
              " 'آب',\n",
              " 'أيلول',\n",
              " 'تشرين',\n",
              " 'دولار',\n",
              " 'دينار',\n",
              " 'ريال',\n",
              " 'درهم',\n",
              " 'ليرة',\n",
              " 'جنيه',\n",
              " 'قرش',\n",
              " 'مليم',\n",
              " 'فلس',\n",
              " 'هللة',\n",
              " 'سنتيم',\n",
              " 'يورو',\n",
              " 'ين',\n",
              " 'يوان',\n",
              " 'شيكل',\n",
              " 'واحد',\n",
              " 'اثنان',\n",
              " 'ثلاثة',\n",
              " 'أربعة',\n",
              " 'خمسة',\n",
              " 'ستة',\n",
              " 'سبعة',\n",
              " 'ثمانية',\n",
              " 'تسعة',\n",
              " 'عشرة',\n",
              " 'أحد',\n",
              " 'اثنا',\n",
              " 'اثني',\n",
              " 'إحدى',\n",
              " 'ثلاث',\n",
              " 'أربع',\n",
              " 'خمس',\n",
              " 'ست',\n",
              " 'سبع',\n",
              " 'ثماني',\n",
              " 'تسع',\n",
              " 'عشر',\n",
              " 'ثمان',\n",
              " 'سبت',\n",
              " 'أحد',\n",
              " 'اثنين',\n",
              " 'ثلاثاء',\n",
              " 'أربعاء',\n",
              " 'خميس',\n",
              " 'جمعة',\n",
              " 'أول',\n",
              " 'ثان',\n",
              " 'ثاني',\n",
              " 'ثالث',\n",
              " 'رابع',\n",
              " 'خامس',\n",
              " 'سادس',\n",
              " 'سابع',\n",
              " 'ثامن',\n",
              " 'تاسع',\n",
              " 'عاشر',\n",
              " 'حادي',\n",
              " 'أ',\n",
              " 'ب',\n",
              " 'ت',\n",
              " 'ث',\n",
              " 'ج',\n",
              " 'ح',\n",
              " 'خ',\n",
              " 'د',\n",
              " 'ذ',\n",
              " 'ر',\n",
              " 'ز',\n",
              " 'س',\n",
              " 'ش',\n",
              " 'ص',\n",
              " 'ض',\n",
              " 'ط',\n",
              " 'ظ',\n",
              " 'ع',\n",
              " 'غ',\n",
              " 'ف',\n",
              " 'ق',\n",
              " 'ك',\n",
              " 'ل',\n",
              " 'م',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ي',\n",
              " 'ء',\n",
              " 'ى',\n",
              " 'آ',\n",
              " 'ؤ',\n",
              " 'ئ',\n",
              " 'أ',\n",
              " 'ة',\n",
              " 'ألف',\n",
              " 'باء',\n",
              " 'تاء',\n",
              " 'ثاء',\n",
              " 'جيم',\n",
              " 'حاء',\n",
              " 'خاء',\n",
              " 'دال',\n",
              " 'ذال',\n",
              " 'راء',\n",
              " 'زاي',\n",
              " 'سين',\n",
              " 'شين',\n",
              " 'صاد',\n",
              " 'ضاد',\n",
              " 'طاء',\n",
              " 'ظاء',\n",
              " 'عين',\n",
              " 'غين',\n",
              " 'فاء',\n",
              " 'قاف',\n",
              " 'كاف',\n",
              " 'لام',\n",
              " 'ميم',\n",
              " 'نون',\n",
              " 'هاء',\n",
              " 'واو',\n",
              " 'ياء',\n",
              " 'همزة',\n",
              " 'ي',\n",
              " 'نا',\n",
              " 'ك',\n",
              " 'كن',\n",
              " 'ه',\n",
              " 'إياه',\n",
              " 'إياها',\n",
              " 'إياهما',\n",
              " 'إياهم',\n",
              " 'إياهن',\n",
              " 'إياك',\n",
              " 'إياكما',\n",
              " 'إياكم',\n",
              " 'إياك',\n",
              " 'إياكن',\n",
              " 'إياي',\n",
              " 'إيانا',\n",
              " 'أولالك',\n",
              " 'تانِ',\n",
              " 'تانِك',\n",
              " 'تِه',\n",
              " 'تِي',\n",
              " 'تَيْنِ',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'ذانِ',\n",
              " 'ذِه',\n",
              " 'ذِي',\n",
              " 'ذَيْنِ',\n",
              " 'هَؤلاء',\n",
              " 'هَاتانِ',\n",
              " 'هَاتِه',\n",
              " 'هَاتِي',\n",
              " 'هَاتَيْنِ',\n",
              " 'هَذا',\n",
              " 'هَذانِ',\n",
              " 'هَذِه',\n",
              " 'هَذِي',\n",
              " 'هَذَيْنِ',\n",
              " 'الألى',\n",
              " 'الألاء',\n",
              " 'أل',\n",
              " 'أنّى',\n",
              " 'أيّ',\n",
              " 'ّأيّان',\n",
              " 'أنّى',\n",
              " 'أيّ',\n",
              " 'ّأيّان',\n",
              " 'ذيت',\n",
              " 'كأيّ',\n",
              " 'كأيّن',\n",
              " 'بضع',\n",
              " 'فلان',\n",
              " 'وا',\n",
              " 'آمينَ',\n",
              " 'آهِ',\n",
              " 'آهٍ',\n",
              " 'آهاً',\n",
              " 'أُفٍّ',\n",
              " 'أُفٍّ',\n",
              " 'أفٍّ',\n",
              " 'أمامك',\n",
              " 'أمامكَ',\n",
              " 'أوّهْ',\n",
              " 'إلَيْكَ',\n",
              " 'إلَيْكَ',\n",
              " 'إليكَ',\n",
              " 'إليكنّ',\n",
              " 'إيهٍ',\n",
              " 'بخٍ',\n",
              " 'بسّ',\n",
              " 'بَسْ',\n",
              " 'بطآن',\n",
              " 'بَلْهَ',\n",
              " 'حاي',\n",
              " 'حَذارِ',\n",
              " 'حيَّ',\n",
              " 'حيَّ',\n",
              " 'دونك',\n",
              " 'رويدك',\n",
              " 'سرعان',\n",
              " 'شتانَ',\n",
              " 'شَتَّانَ',\n",
              " 'صهْ',\n",
              " 'صهٍ',\n",
              " 'طاق',\n",
              " 'طَق',\n",
              " 'عَدَسْ',\n",
              " 'كِخ',\n",
              " 'مكانَك',\n",
              " 'مكانَك',\n",
              " 'مكانَك',\n",
              " 'مكانكم',\n",
              " 'مكانكما',\n",
              " 'مكانكنّ',\n",
              " 'نَخْ',\n",
              " 'هاكَ',\n",
              " 'هَجْ',\n",
              " 'هلم',\n",
              " 'هيّا',\n",
              " 'هَيْهات',\n",
              " 'وا',\n",
              " 'واهاً',\n",
              " 'وراءَك',\n",
              " 'وُشْكَانَ',\n",
              " 'وَيْ',\n",
              " 'يفعلان',\n",
              " 'تفعلان',\n",
              " 'يفعلون',\n",
              " 'تفعلون',\n",
              " 'تفعلين',\n",
              " 'اتخذ',\n",
              " 'ألفى',\n",
              " 'تخذ',\n",
              " 'ترك',\n",
              " 'تعلَّم',\n",
              " 'جعل',\n",
              " 'حجا',\n",
              " 'حبيب',\n",
              " 'خال',\n",
              " 'حسب',\n",
              " 'خال',\n",
              " 'درى',\n",
              " 'رأى',\n",
              " 'زعم',\n",
              " 'صبر',\n",
              " 'ظنَّ',\n",
              " 'عدَّ',\n",
              " 'علم',\n",
              " 'غادر',\n",
              " 'ذهب',\n",
              " 'وجد',\n",
              " 'ورد',\n",
              " 'وهب',\n",
              " 'أسكن',\n",
              " 'أطعم',\n",
              " 'أعطى',\n",
              " 'رزق',\n",
              " 'زود',\n",
              " 'سقى',\n",
              " 'كسا',\n",
              " 'أخبر',\n",
              " 'أرى',\n",
              " 'أعلم',\n",
              " 'أنبأ',\n",
              " 'حدَث',\n",
              " 'خبَّر',\n",
              " 'نبَّا',\n",
              " 'أفعل به',\n",
              " 'ما أفعله',\n",
              " 'بئس',\n",
              " 'ساء',\n",
              " 'طالما',\n",
              " 'قلما',\n",
              " 'لات',\n",
              " 'لكنَّ',\n",
              " 'ءَ',\n",
              " 'أجل',\n",
              " 'إذاً',\n",
              " 'أمّا',\n",
              " 'إمّا',\n",
              " 'إنَّ',\n",
              " 'أنًّ',\n",
              " 'أى',\n",
              " 'إى',\n",
              " 'أيا',\n",
              " 'ب',\n",
              " 'ثمَّ',\n",
              " 'جلل',\n",
              " 'جير',\n",
              " 'رُبَّ',\n",
              " 'س',\n",
              " 'علًّ',\n",
              " 'ف',\n",
              " 'كأنّ',\n",
              " 'كلَّا',\n",
              " 'كى',\n",
              " 'ل',\n",
              " 'لات',\n",
              " 'لعلَّ',\n",
              " 'لكنَّ',\n",
              " 'لكنَّ',\n",
              " 'م',\n",
              " 'نَّ',\n",
              " 'هلّا',\n",
              " 'وا',\n",
              " 'أل',\n",
              " 'إلّا',\n",
              " 'ت',\n",
              " 'ك',\n",
              " 'لمّا',\n",
              " 'ن',\n",
              " 'ه',\n",
              " 'و',\n",
              " 'ا',\n",
              " 'ي',\n",
              " 'تجاه',\n",
              " 'تلقاء',\n",
              " 'جميع',\n",
              " 'حسب',\n",
              " 'سبحان',\n",
              " 'شبه',\n",
              " 'لعمر',\n",
              " 'مثل',\n",
              " 'معاذ',\n",
              " 'أبو',\n",
              " 'أخو',\n",
              " 'حمو',\n",
              " 'فو',\n",
              " 'مئة',\n",
              " 'مئتان',\n",
              " 'ثلاثمئة',\n",
              " 'أربعمئة',\n",
              " 'خمسمئة',\n",
              " 'ستمئة',\n",
              " 'سبعمئة',\n",
              " 'ثمنمئة',\n",
              " 'تسعمئة',\n",
              " 'مائة',\n",
              " 'ثلاثمائة',\n",
              " 'أربعمائة',\n",
              " 'خمسمائة',\n",
              " 'ستمائة',\n",
              " 'سبعمائة',\n",
              " 'ثمانمئة',\n",
              " 'تسعمائة',\n",
              " 'عشرون',\n",
              " 'ثلاثون',\n",
              " 'اربعون',\n",
              " 'خمسون',\n",
              " 'ستون',\n",
              " 'سبعون',\n",
              " 'ثمانون',\n",
              " 'تسعون',\n",
              " 'عشرين',\n",
              " 'ثلاثين',\n",
              " 'اربعين',\n",
              " 'خمسين',\n",
              " 'ستين',\n",
              " 'سبعين',\n",
              " 'ثمانين',\n",
              " 'تسعين',\n",
              " 'بضع',\n",
              " 'نيف',\n",
              " 'أجمع',\n",
              " 'جميع',\n",
              " 'عامة',\n",
              " 'عين',\n",
              " 'نفس',\n",
              " 'لا سيما',\n",
              " 'أصلا',\n",
              " 'أهلا',\n",
              " 'أيضا',\n",
              " 'بؤسا',\n",
              " 'بعدا',\n",
              " 'بغتة',\n",
              " 'تعسا',\n",
              " 'حقا',\n",
              " 'حمدا',\n",
              " 'خلافا',\n",
              " 'خاصة',\n",
              " 'دواليك',\n",
              " 'سحقا',\n",
              " 'سرا',\n",
              " 'سمعا',\n",
              " 'صبرا',\n",
              " 'صدقا',\n",
              " 'صراحة',\n",
              " 'طرا',\n",
              " 'عجبا',\n",
              " 'عيانا',\n",
              " 'غالبا',\n",
              " 'فرادى',\n",
              " 'فضلا',\n",
              " 'قاطبة',\n",
              " 'كثيرا',\n",
              " 'لبيك',\n",
              " 'معاذ',\n",
              " 'أبدا',\n",
              " 'إزاء',\n",
              " 'أصلا',\n",
              " 'الآن',\n",
              " 'أمد',\n",
              " 'أمس',\n",
              " 'آنفا',\n",
              " 'آناء',\n",
              " 'أنّى',\n",
              " 'أول',\n",
              " 'أيّان',\n",
              " 'تارة',\n",
              " 'ثمّ',\n",
              " 'ثمّة',\n",
              " 'حقا',\n",
              " 'صباح',\n",
              " 'مساء',\n",
              " 'ضحوة',\n",
              " 'عوض',\n",
              " 'غدا',\n",
              " 'غداة',\n",
              " 'قطّ',\n",
              " 'كلّما',\n",
              " 'لدن',\n",
              " 'لمّا',\n",
              " 'مرّة',\n",
              " 'قبل',\n",
              " 'خلف',\n",
              " 'أمام',\n",
              " 'فوق',\n",
              " 'تحت',\n",
              " 'يمين',\n",
              " 'شمال',\n",
              " 'ارتدّ',\n",
              " 'استحال',\n",
              " 'أصبح',\n",
              " 'أضحى',\n",
              " 'آض',\n",
              " 'أمسى',\n",
              " 'انقلب',\n",
              " 'بات',\n",
              " 'تبدّل',\n",
              " 'تحوّل',\n",
              " 'حار',\n",
              " 'رجع',\n",
              " 'راح',\n",
              " 'صار',\n",
              " 'ظلّ',\n",
              " 'عاد',\n",
              " 'غدا',\n",
              " 'كان',\n",
              " 'ما انفك',\n",
              " 'ما برح',\n",
              " 'مادام',\n",
              " 'مازال',\n",
              " 'مافتئ',\n",
              " 'ابتدأ',\n",
              " 'أخذ',\n",
              " 'اخلولق',\n",
              " 'أقبل',\n",
              " 'انبرى',\n",
              " 'أنشأ',\n",
              " 'أوشك',\n",
              " 'جعل',\n",
              " 'حرى',\n",
              " 'شرع',\n",
              " 'طفق',\n",
              " 'علق',\n",
              " 'قام',\n",
              " 'كرب',\n",
              " 'كاد',\n",
              " 'هبّ']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "CxDZyznII1X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "WGD_YlqkI45I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnXcEi5pI46s",
        "outputId": "b9d22696-15a7-4c84-850c-f31273aa2a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "y9KVFXAgI4_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjZl7JyUJKFS",
        "outputId": "5fdcfc6c-4414-46b2-cac8-0a3182c451f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i three vision india .',\n",
              " 'in 3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'from alexand onward , greek , turk , mogul , portugues , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation .',\n",
              " 'we conquer anyon .',\n",
              " 'we grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'i believ india got first vision 1857 , start war independ .',\n",
              " 'it freedom must protect nurtur build .',\n",
              " 'if free , one respect us .',\n",
              " 'my second vision india ’ develop .',\n",
              " 'for fifti year develop nation .',\n",
              " 'it time see develop nation .',\n",
              " 'we among top 5 nation world term gdp .',\n",
              " 'we 10 percent growth rate area .',\n",
              " 'our poverti level fall .',\n",
              " 'our achiev global recognis today .',\n",
              " 'yet lack self-confid see develop nation , self-reli self-assur .',\n",
              " 'isn ’ incorrect ?',\n",
              " 'i third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus i believ unless india stand world , one respect us .',\n",
              " 'onli strength respect strength .',\n",
              " 'we must strong militari power also econom power .',\n",
              " 'both must go hand-in-hand .',\n",
              " 'my good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'i lucki work three close consid great opportun life .',\n",
              " 'i see four mileston career']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "PITTEiFLJQcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "D4Nhdhm8JUwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1iRAOQiJUyK",
        "outputId": "98827146-8422-4383-e82e-79446bd72bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three vision india .',\n",
              " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , came loot us , took .',\n",
              " 'yet done nation .',\n",
              " 'conquer anyon .',\n",
              " 'grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'believ india got first vision 1857 , start war independ .',\n",
              " 'freedom must protect nurtur build .',\n",
              " 'free , one respect us .',\n",
              " 'second vision india ’ develop .',\n",
              " 'fifti year develop nation .',\n",
              " 'time see develop nation .',\n",
              " 'among top 5 nation world term gdp .',\n",
              " '10 percent growth rate area .',\n",
              " 'poverti level fall .',\n",
              " 'achiev global recogni today .',\n",
              " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
              " '’ incorrect ?',\n",
              " 'third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus believ unless india stand world , one respect us .',\n",
              " 'on strength respect strength .',\n",
              " 'must strong militari power also econom power .',\n",
              " 'must go hand-in-hand .',\n",
              " 'good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'lucki work three close consid great opportun life .',\n",
              " 'see four mileston career']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "wCktBvhHJcy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply lemmatization\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    #sentences[i]=sentences[i].lower()\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "7dnsLthRJc0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hTBYNS1JqAu",
        "outputId": "eabfcb00-8426-4f9a-d052-5368d90a58ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['three vision india .',\n",
              " '3000 year histori , peopl world come invad us , captur land , conquer mind .',\n",
              " 'alexand onward , greek , turk , mogul , portugu , british , french , dutch , come loot us , take .',\n",
              " 'yet do nation .',\n",
              " 'conquer anyon .',\n",
              " 'grab land , cultur , histori tri enforc way life .',\n",
              " 'whi ?',\n",
              " 'becaus respect freedom others.that first vision freedom .',\n",
              " 'believ india get first vision 1857 , start war independ .',\n",
              " 'freedom must protect nurtur build .',\n",
              " 'free , one respect us .',\n",
              " 'second vision india ’ develop .',\n",
              " 'fifti year develop nation .',\n",
              " 'time see develop nation .',\n",
              " 'among top 5 nation world term gdp .',\n",
              " '10 percent growth rate area .',\n",
              " 'poverti level fall .',\n",
              " 'achiev global recogni today .',\n",
              " 'yet lack self-confid see develop nation , self-r self-assur .',\n",
              " '’ incorrect ?',\n",
              " 'third vision .',\n",
              " 'india must stand world .',\n",
              " 'becaus believ unless india stand world , one respect us .',\n",
              " 'strength respect strength .',\n",
              " 'must strong militari power also econom power .',\n",
              " 'must go hand-in-hand .',\n",
              " 'good fortun work three great mind .',\n",
              " 'dr. vikram sarabhai dept .',\n",
              " 'space , professor satish dhawan , succeed dr. brahm prakash , father nuclear materi .',\n",
              " 'lucki work three close consid great opportun life .',\n",
              " 'see four mileston career']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}